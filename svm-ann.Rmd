---
title: "Stat 218 - Analytics Project V"
author: "Inigo Benavides and Rommel Bartolome"
date: "May 20, 2019"
abstract: "In this project, we utilized two machine learning techniques involving a dataset of subscribers of a cellular network where we will determine if the particular customer churns or not. First, a Support Vector Machine (SVM) was implemented and was found that the all kernels have similar accuracy but we will choose a radial one, as it has a slightly higher specificity. For SVM, we were able to accurately predict 74% of the customers that churned. Second, an Artificial Neural Network (ANN) was also utilized. Ultimately, it gave us a similar accuracy of 74%, even with varying values."
output:
  pdf_document: default
  html_document: default
---

# Introduction

For our last analytics project, we will be utilizing two machine learning techniques called Support Vector Machines (SVM) and Artificial Neural Networks (ANN). We will implement these techniques to cellular customer data. The dataset given is a collection of 2000 subscribers of a cellular network. We will predict whether a subscriber will churn or not. It should be noted that the Churn variable signifies whether the customer had left the company two months after observation.

# Data Loading and Cleaning

We load all the libraries we will be using in this project. In addition, similar to our previous projects, we will also clean our data and set our seed for reproducibility. 

First, we will generate new predictors and use a "one-hot encoding" for the categorical variables, i.e. generate new features/predictors that are binary for each category. For example, in encoding NonUSTravel, we will make new variables called NonUSTravel_No and NonUSTravel_Yes. Also, we will standardize the numerical variables. 

```{r message=FALSE, warning=FALSE}
#knitr::opts_chunk$set(cache = TRUE)
library(tidyverse)
library(e1071)
library(caret)

binary_encoder <- function(x) {
  case_when(
    x == "Yes" ~ 1,
    TRUE ~ 0
  )
}

seed <- 1
df <- read_csv("data_BaBe.csv") 

## create features for category variables
set.seed(seed)
train_size <- 1500
train_index <- sample(seq_len(nrow(df)), size=train_size)

df_process <- df %>% 
  mutate(Churn=binary_encoder(Churn),
         MaritalStatus=binary_encoder(MaritalStatus),
         OwnsComputer=binary_encoder(OwnsComputer),
         NonUSTravel=binary_encoder(NonUSTravel),
         RespondsToMailOffers=binary_encoder(RespondsToMailOffers),
         ChildrenInHH=binary_encoder(ChildrenInHH))

# One-hot encode occupation
occupation_dummies <- dummyVars(" ~ .", data=df_process)
df_process <- occupation_dummies %>%
  predict(newdata=df_process) %>%
  as.data.frame
```

Now, we will divide the dataset into training and test datasets,  with the training dataset as the 1500 randomly selected observations (out of 2000) and the remaining 500 data points as the test dataset:

```{r}
# Separate into train and test
df_train <- df_process[train_index, ]
df_test <- df_process[-train_index, ]

# Separate X and y for train and test sets
X_train <- df_train %>% select(-Churn)
y_train <- df_train$Churn
X_test <- df_test %>% select(-Churn)
y_test <- df_test$Churn

df_train %>% head %>% as_tibble()
```

# Exploratory Data Analysis

Before building our model, we first do some Exploratory Data Analysis (EDA) to our original dataset to better understand it. First, we wish to determine the class distribution of the target. We note that there are 505 "Yes" observations and 1,495 "No" observations. To respect the class distribution in model building, we stratify split according to this distribution.

```{r}
df %>% group_by(Churn) %>% summarise(n=n())
```

Here we can see than we have 505 customers that churned while 1,495 did not. Next, we inspect the numerical features and run horizontal box plots to see the distribution against the target.

```{r}
# Boxplot of numerics against target distribution
numerics <- unlist(lapply(df, is.numeric))
numerics[1] <- TRUE # Set Churn column to be included
df[, numerics] %>% gather(key="feature", value="value", -Churn) %>% 
  ggplot(aes(x=reorder(feature, value, FUN=var), y=value, color=Churn)) + geom_boxplot() +
  theme_minimal() +
  coord_flip() +
  labs(title="Distribution of numeric features") +
  ylab("Feature")
```

Based on the above analysis, it appears that there may be large discriminative power in `MonthlyMinutes` and `PercChangeMinutes`. The percentage change in minutes of use appears especially discriminative, and we can consider this intuitive because lower usage in the service would be indicative of lower future usage and eventually churn.

Next, we can plot the correlation matrix of these numeric covariates:

```{r}
# Correlation matrix
library(reshape2)
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

df_numeric <- df[,numerics]
df_numeric %>%
  mutate(Churn=case_when(Churn=="Yes" ~ 1, TRUE ~ 0)) %>% 
  cor %>%
  get_upper_tri() %>% 
  melt(na.rm=TRUE) %>% ggplot(aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color="white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, 
    size = 7, hjust = 1),
    axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank()) +
  coord_fixed()

```

Next, we inspect the categoricals and plot the distribution against the target variable:

```{r warning=FALSE}
# Inspect categoricals
df %>% select_if(negate(is.numeric)) %>% summarise_each(funs(n_distinct)) %>% as_tibble()
``` 

We find that all categoricals except `Occupation` are binary. We can encode these as 1 or 0 instead and also plot the correlation matrix.

```{r}
# 
df %>%
  select_if(negate(is.numeric)) %>% 
  select(-Occupation) %>% 
  mutate_all(binary_encoder) %>% 
  cor %>%
  get_upper_tri() %>% 
  melt(na.rm=TRUE) %>% ggplot(aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color="white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, 
    size = 7, hjust = 1),
    axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank()) +
  coord_fixed()
```

There do not seem to be any strong categorical correlations with `Churn`.

# Support Vector Machines

For the SVM part, we will try different kernels and check what kernel will be the best in classifying if a customer will churn. Our goal is to be able to have an error rate of $\leq$ 10%. 

First, we will fit an SVM with a radial kernel, and tuning it by finding the best cost and gamma features:

```{r}
library(pROC)
set.seed(seed)
df_train[,1] <- as.factor(df_train[,1])
df_test[,1] <- as.factor(df_test[,1])
svm_radial <- tune(svm, Churn ~., data = df_train, kernel = "radial",
                   ranges = list(cost = c(0.01, 1, 10),
                                 gamma = c(0.01, 1, 10)))
svm_radial$best.model
confusionMatrix(predict(svm_radial$best.model, df_test), 
                df_test$Churn)
```

Surprisingly, our model classified almost all of it as "Not Churn" with only one classified to be "Churn". We check if this will also be the case for a linear kernel:

```{r}
set.seed(100)
svm_linear <- tune(svm, Churn~., data = df_train, kernel="linear", ranges = list(cost = c(0.01,1,10),
                                                                                   gamma = c(0.01,1,10)))
svm_linear$best.model
confusionMatrix(predict(svm_linear$best.model, df_test), df_test$Churn)
```

Again and worse, it classified all of it as not churning. We try for a polynomial kernel:

```{r}
set.seed(seed)
svm_poly <- tune(svm, Churn~., data = df_train, kernel="polynomial",ranges = list(cost = c(0.01,1,10),
                                                                                   degree = c(2,3,4)))
svm_poly$best.model
confusionMatrix(predict(svm_poly$best.model, df_test), df_test$Churn)

#df_train[,1] <- as.numeric(df_train[,1])
#df_test[,1] <- as.numeric(df_test[,1])
```

Again, the predictions are all NOT churn. Overall, the accuracy is at 74% but for this exercise, we will choose a radial one as it has a slightly higher specificity.


# Artificial Neural Networks

Now, we will fit an Artificial Neural Network. We try to create one with a 3 hidden layers.

```{r warning=FALSE}
library(neuralnet)
fit_nn <- neuralnet(Churn~., data=df_train, hidden=3)
plot(fit_nn)
tune_nn <- caret::train(Churn~.,
                        data=df_train,
                        method='mlpWeightDecayML',
                        tuneGrid=expand.grid(layer1 = c(2:5),
                                             layer2 = c(1:3),
                                             layer3 = c(0:1),
                                             decay=0.1),
                        verbose=TRUE)

tune_nn

```

Here we see that for layer 1 we used 2 nodes, for layer 2 we used 1 node and none for layer 3. We check the performance of this model:

```{r warning=FALSE}
confusionMatrix(predict(tune_nn, df_test), df_test$Churn)
```

Here, we see the same performance as above, where most of the predictions were NOT Churn. We check a similar 2-layer ANN, with varying decay value:

```{r warning=FALSE}
fit_nn_2 <- neuralnet(Churn~., data=df_train, hidden=2)
plot(fit_nn_2)
tune_nn_2 <- caret::train(Churn~.,
                        data=df_train,
                        method='mlpWeightDecayML',
                        tuneGrid=expand.grid(layer1 = c(2:5),
                                             layer2 = c(0:2),
                                             layer3 = c(0),
                                             decay=c(0.001, 0.1, 1)),
                        verbose=TRUE)

tune_nn_2
```

Now we have layer 1 with two nodes, the decay being 0.001. We now check its performance:

```{r warning=FALSE}
confusionMatrix(predict(tune_nn_2, df_test), df_test$Churn)
```

Unfortunately, this is the case again. We also tried different models to the dataset, but unfortunately, this has also been the case. 

What we can infer is that the features that we were given were not enough to identify if a customer will Churn or Not Churn, with an error rate of $\leq$ 10%. All the models classified most of the data as "not churn".

## Conclusions

For the SVM technique, we have the following accuracy metrics:

| **Model**                       |**Accuracy** | 
|---------------------------------|-------------|
| Radial Kernel                   | 74%         |
| Linear Kernel                   | 74%         |
| Polynomial Kernel               | 74%         |

For the ANN technique, we have the following accuracy metrics:

| **Model**                       |**Accuracy** | 
|---------------------------------|-------------|
| ANN - 3 Layers                  | 74%         |
| ANN - 2 Layers                  | 74%         |


Unfortunately, we were not able to lower the error rate to $\leq$ 10%, but only to 26%. This may mean that the features in the data were not able to capture what a customer that "Churn"

